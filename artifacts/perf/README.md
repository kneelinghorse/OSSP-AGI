# Performance Telemetry Logs

This directory contains performance telemetry data generated by workbench operations.

## Log Format

Performance logs are written in **JSONL** (JSON Lines) format, with one JSON object per line. Plain `.log` files are no longer used for CI budget evaluation.

### Entry Schema

Each log entry should contain:

```json
{
  "tool": "string",      // Required: Tool/service name (e.g., "discovery", "mcp", "registry")
  "step": "string",      // Required: Operation/step identifier (e.g., "resolve", "get", "tool_exec")
  "ms": number,          // Required: Duration in milliseconds
  "ok": boolean,         // Optional: Success flag (default: true)
  "message": "string",   // Optional: Human-readable description
  "context": {}          // Optional: Additional metadata
}
```

### Example Entries

```jsonl
{"tool":"discovery","step":"resolve","ms":42.5,"ok":true,"message":"URN resolution completed"}
{"tool":"mcp","step":"tool_exec","ms":125.3,"ok":true,"message":"Tool execution successful"}
{"tool":"registry","step":"get","ms":15.2,"ok":true,"message":"Registry fetch completed"}
```

## Log File Naming

- **Pattern**: `{YYYY-MM-DD}/{session-id}.jsonl`
- **Location**: `artifacts/perf/{date}/{session}.jsonl`

Example: `artifacts/perf/2025-10-24/wsap-1729785600000.jsonl`

## Metrics Collection

The performance pipeline (`src/metrics/perf.js`) scans this directory for:
- Files with `performance` or `metrics` in the name
- Files with a `.jsonl` extension

### Summary JSON

Running `node cli/index.js perf:status --format json` produces `.artifacts/perf/summary.json` with:

- `discovery`, `mcp`, and `system` metric aggregates
- `sourceLogs`: array of `{ absolute, relative }` paths used for the summary
- `latestLog`: absolute path to the newest JSONL file that contributed data

CI copies `latestLog` to `.artifacts/perf/latest.jsonl` so downstream jobs can parse live metrics directly from the telemetry stream.

## Budget Enforcement

Performance budgets are enforced in CI via `scripts/ci/perf-budget.js`:

- **Discovery P95**: ≤ 1000ms
- **MCP P95**: ≤ 3000ms
- **Registry GET P95**: ≤ 150ms
- **Resolve P95**: ≤ 300ms

## No Mock Data Policy

**As of Sprint 22**, the performance pipeline no longer generates mock data. All metrics must come from real execution logs:

- ❌ **Removed**: `seedMockPerfData()` function
- ✅ **Required**: Tests/workbench runs must generate telemetry

### If you see "missing perf data" errors:

1. Ensure your tests execute real workbench operations
2. Check that log files are being written to `artifacts/perf/`
3. Verify JSONL entries match the schema above
4. Confirm log files are less than 1 hour old

## Generating Telemetry in Tests

Example pattern for test instrumentation:

```javascript
import { appendFile } from 'node:fs/promises';
import path from 'node:path';

async function logPerfMetric(tool, step, duration, ok = true) {
  const logDir = path.join(process.cwd(), 'artifacts/perf');
  const logFile = path.join(logDir, 'test-metrics.jsonl');
  
  const entry = {
    tool,
    step,
    ms: duration,
    ok,
    timestamp: new Date().toISOString(),
  };
  
  await appendFile(logFile, JSON.stringify(entry) + '\n', 'utf8');
}

// Usage in tests:
test('discovery operation', async () => {
  const start = Date.now();
  await performDiscovery();
  const duration = Date.now() - start;
  
  await logPerfMetric('discovery', 'resolve', duration, true);
});
```

## CI Integration

The CI pipeline:
1. Runs tests (which generate telemetry logs)
2. Collects metrics: `node cli/index.js perf:status --format json` (writes `.artifacts/perf/summary.json`)
3. Copies the latest JSONL log to `.artifacts/perf/latest.jsonl` (path is also exposed via `summary.sourceLogs`)
4. Enforces budgets: `node scripts/ci/perf-budget.js --file .artifacts/perf/summary.json`
5. Surfaces live metrics in CI summary by parsing `.artifacts/perf/latest.jsonl` directly

See `.github/workflows/ci.yml` for the complete flow.
